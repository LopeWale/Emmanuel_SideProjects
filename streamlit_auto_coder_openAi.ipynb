{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "streamlit_auto_coder_openAi.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNKwacgB/EymM15nYFryizU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LopeWale/Emmanuel_SideProjects/blob/main/streamlit_auto_coder_openAi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "06LceedbkP7W",
        "outputId": "9758f2b4-20f1-4bb7-af7f-a61b570c1d00"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9b9b989d842d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWEIGHTS_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertForMaskedLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRobertaConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRobertaForMaskedLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXLMRobertaConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXLMRobertaForMaskedLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXLMRobertaTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "\"\"\"\n",
        "Create an automated coding system that has an user interface using streamlit and uses openai codex model api to generate python code based on the prompt given by the user\n",
        "\"\"\"\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler, TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from transformers import WEIGHTS_NAME, BertConfig, BertForMaskedLM, BertTokenizer\n",
        "from transformers import RobertaConfig, RobertaForMaskedLM, RobertaTokenizer\n",
        "from transformers import XLMRobertaConfig, XLMRobertaForMaskedLM, XLMRobertaTokenizer\n",
        "from transformers import XLMConfig, XLMForMaskedLM, XLMTokenizer\n",
        "from transformers import XLNetConfig, XLNetForMaskedLM, XLNetTokenizer\n",
        "\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "import streamlit as st\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "#PYTHON_CODE = \"int main(){\\n\\tint x=5;\\n\\tint y=6;\\n\\tint z;\\n\\tz=x+y;\\n\\treturn z;\\n}\"\n",
        "#PYTHON_CODE = \"import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport warnings\\nwarnings.filterwarnings('ignore')\\ndata = pd.read_csv('../input/train.csv')\\ndata.head(5)\"\n",
        "PYTHON_CODE = \"import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport warnings\\nwarnings.filterwarnings('ignore')\"\n",
        "\n",
        "def get_input_code(text):\n",
        "    if text == \"\":\n",
        "        return PYTHON_CODE\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "def get_predictions(text):\n",
        "    return generate_code(text)\n",
        "\n",
        "def generate_code(text):\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    ## Required parameters\n",
        "    parser.add_argument(\"--model_type\", default=None, type=str, required=True,\n",
        "                        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()))\n",
        "    parser.add_argument(\"--model_name_or_path\", default=None, type=str, required=True,\n",
        "                        help=\"Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(ALL_MODELS))\n",
        "    parser.add_argument(\"--output_dir\", default=None, type=str, required=True,\n",
        "                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
        "\n",
        "    ## Other parameters\n",
        "    parser.add_argument(\"--prompt\", type=str, default=\"\")\n",
        "    parser.add_argument(\"--length\", type=int, default=20)\n",
        "    parser.add_argument(\"--stop_token\", type=str, default=None,\n",
        "                        help=\"Token at which text generation is stopped\")\n",
        "\n",
        "    parser.add_argument(\"--temperature\", type=float, default=1.0,\n",
        "                        help=\"temperature of 0 implies greedy sampling\")\n",
        "    parser.add_argument(\"--repetition_penalty\", type=float, default=1.0,\n",
        "                        help=\"primarily useful for CTRL model; in that case, use 1.2\")\n",
        "    parser.add_argument(\"--k\", type=int, default=0)\n",
        "    parser.add_argument(\"--p\", type=float, default=0.9)\n",
        "\n",
        "    parser.add_argument(\"--num_return_sequences\", type=int, default=1)\n",
        "\n",
        "    parser.add_argument('--seed', type=int, default=42)\n",
        "    parser.add_argument('--no_cuda', action='store_true',\n",
        "                        help=\"Avoid using CUDA when available\")\n",
        "    parser.add_argument('--num_samples', type=int, default=20)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    args.device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "\n",
        "    set_seed(args)\n",
        "\n",
        "    args.model_type = args.model_type.lower()\n",
        "    model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "    tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path)\n",
        "    model = model_class.from_pretrained(args.model_name_or_path)\n",
        "    model.to(args.device)\n",
        "    model.eval()\n",
        "\n",
        "    if args.length < 0 and model.config.max_position_embeddings > 0:\n",
        "        args.length = model.config.max_position_embeddings\n",
        "    elif 0 < model.config.max_position_embeddings < args.length:\n",
        "        args.length = model.config.max_position_embeddings  # No generation bigger than model size\n",
        "    elif args.length < 0:\n",
        "        args.length = MAX_LENGTH  # avoid infinite loop\n",
        "\n",
        "    logger.info(args)\n",
        "\n",
        "    if args.model_type in [\"ctrl\"]:\n",
        "        if args.temperature > 0.7:\n",
        "            logger.info('CTRL typically works better with lower temperatures (and lower top_k).')\n",
        "\n",
        "    while True:\n",
        "        raw_text = text\n",
        "        context_tokens = tokenizer.encode(raw_text, add_special_tokens=False)\n",
        "        if args.model_type in ['xlnet', 'xlm']:\n",
        "            # XLNet is a direct (predict same token, not next token) and bi-directional model by default\n",
        "            # => need one additional dummy token in the input (will be masked), attention mask and target mapping (see model docstring)\n",
        "            context_tokens = tokenizer.build_inputs_with_special_tokens(context_tokens)\n",
        "            input_ids = torch.tensor([context_tokens], dtype=torch.long).to(args.device)\n",
        "            input_ids = input_ids.repeat(args.num_samples, 1)\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "            attention_mask = torch.zeros_like(input_ids)\n",
        "            lm_labels = torch.full_like(input_ids, fill_value=-1)\n",
        "            lm_labels[:, -1] = torch.tensor(context_tokens, dtype=torch.long).to(args.device)\n",
        "            lm_labels = lm_labels.repeat(args.num_samples, 1)\n",
        "\n",
        "        else:\n",
        "            assert args.model_type in [\"bert\", \"roberta\", \"distilbert\", \"camembert\", \"xlmroberta\"]\n",
        "            # Models with a LM head are next token prediction (classification) models by default\n",
        "            # when used with the cross entropy loss. Last token prediction is always treated as\n",
        "            # token generation.\n",
        "            input_ids = torch.tensor([context_tokens], dtype=torch.long).to(args.device)\n",
        "            input_ids = input_ids.repeat(args.num_samples, 1)\n",
        "            lm_labels = None\n",
        "\n",
        "            # if model_type in [\"bert\", \"roberta\", \"distilbert\", \"camembert\", \"xlmroberta\"]:\n",
        "            #     # Models with a LM head are next token prediction (classification) models by default\n",
        "            #     # when used with the cross entropy loss. Last token prediction is always treated as\n",
        "            #     # token generation.\n",
        "            #     lm_labels = torch.full_like(input_ids, fill_value=-1)\n",
        "            #     lm_labels[:, -1] = torch.tensor(context_tokens, dtype=torch.long).to(args.device)\n",
        "            #     lm_labels = lm_labels.repeat(args.num_samples, 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, lm_labels=lm_labels) if args.model_type in [\"bert\", \"roberta\", \"distilbert\", \"camembert\", \"xlmroberta\"] else model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, lm_labels=lm_labels)\n",
        "            loss = outputs[0]\n",
        "            logits = outputs[0] if args.model_type in ['bert', 'xlnet'] else outputs[1]\n",
        "            logits = logits[:, -1, :] / (args.temperature if args.temperature > 0 else 1.)\n",
        "            logits = top_k_top_p_filtering(logits, top_k=args.k, top_p=args.p)\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "            prev = torch.topk(probs, 1)[1] if args.no_cuda else torch.topk(probs, 1)[1].to('cpu')\n",
        "            if i >= args.length:\n",
        "                break\n",
        "            current_output = torch.cat((prev, current_output), dim=1)\n",
        "\n",
        "        out = current_output.tolist()\n",
        "        for i in range(args.num_samples):\n",
        "            text = tokenizer.decode(out[i], clean_up_tokenization_spaces=True)\n",
        "            text = text[: text.find(args.stop_token) if args.stop_token else None]\n",
        "\n",
        "            return text\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "    \"\"\"\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # Back to unsorted indices and set them to -infinity\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    'bert': (BertConfig, BertForMaskedLM, BertTokenizer),\n",
        "    'xlnet': (XLNetConfig, XLNetForMaskedLM, XLNetTokenizer),\n",
        "    'xlm': (XLMConfig, XLMForMaskedLM, XLMTokenizer),\n",
        "    'roberta': (RobertaConfig, RobertaForMaskedLM, RobertaTokenizer),\n",
        "    'distilbert': (DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer),\n",
        "    'camembert': (CamembertConfig, CamembertForMaskedLM, CamembertTokenizer),\n",
        "    'xlmroberta': (XLMRobertaConfig, XLMRobertaForMaskedLM, XLMRobertaTokenizer),\n",
        "}\n",
        "\n",
        "ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) for conf in (BertConfig, XLNetConfig, XLMConfig,\n",
        "                                                                                RobertaConfig, DistilBertConfig,\n",
        "                                                                                CamembertConfig, XLMRobertaConfig)), ())\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    'bert': (BertConfig, BertForMaskedLM, BertTokenizer),\n",
        "    'xlnet': (XLNetConfig, XLNetForMaskedLM, XLNetTokenizer),\n",
        "    'xlm': (XLMConfig, XLMForMaskedLM, XLMTokenizer),\n",
        "    'roberta': (RobertaConfig, RobertaForMaskedLM, RobertaTokenizer),\n",
        "    'distilbert': (DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer),\n",
        "    'camembert': (CamembertConfig, CamembertForMaskedLM, CamembertTokenizer),\n",
        "    'xlmroberta': (XLMRobertaConfig, XLMRobertaForMaskedLM, XLMRobertaTokenizer),\n",
        "}\n",
        "\n",
        "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n"
      ],
      "metadata": {
        "id": "ByvnS047kf1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    st.title(\"Automated Code Generator\")\n",
        "    st.subheader(\"Python Code Generator\")\n",
        "    st.markdown(\"This application is a Streamlit dashboard that can be used to generate Python code using OpenAI's GPT-3 API.\")\n",
        "\n",
        "    st.sidebar.title(\"Generate Python Code\")\n",
        "    st.sidebar.markdown(\"This application is a Streamlit dashboard that can be used to generate Python code using OpenAI's GPT-3 API.\")\n",
        "    st.sidebar.markdown(\"Enter your code snippet below:\")\n",
        "\n",
        "    code_input = st.sidebar.text_area(\"\", height=200)\n",
        "    user_input = get_input_code(code_input)\n",
        "    #st.write(f\"Inputted Code: {user_input}\")\n",
        "\n",
        "    if st.sidebar.checkbox(\"Show Input Code\"):\n",
        "        st.subheader(\"Inputted Code\")\n",
        "        st.code(user_input)\n",
        "\n",
        "    st.sidebar.markdown(\"Enter the prompt below:\")\n",
        "    prompt_input = st.sidebar.text_area(\"\", height=50)\n",
        "    user_prompt = get_input_code(prompt_input)\n",
        "    #st.write(f\"Prompt: {user_prompt}\")\n",
        "\n",
        "    if st.sidebar.checkbox(\"Show Prompt\"):\n",
        "        st.subheader(\"Prompt\")\n",
        "        st.code(user_prompt)\n",
        "\n",
        "    if st.sidebar.button(\"Generate Code\"):\n",
        "        st.subheader(\"Generated Code\")\n",
        "        with st.spinner('Generating Code...'):\n",
        "            time.sleep(5)\n",
        "            generated_code = get_predictions(user_prompt)\n",
        "            #st.write(f\"Generated Code: {generated_code}\")\n",
        "            st.code(generated_code)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "pytVoDN2lJXM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}